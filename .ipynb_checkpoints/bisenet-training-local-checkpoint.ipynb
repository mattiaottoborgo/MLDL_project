{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8358506,"sourceType":"datasetVersion","datasetId":4967162},{"sourceId":8358521,"sourceType":"datasetVersion","datasetId":4967173},{"sourceId":8358637,"sourceType":"datasetVersion","datasetId":4951893}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#execute just the first time to load scripts and datset\nimport sys\nfrom shutil import copytree,copyfile\nsys.path.append( \"/kaggle/input/cityscapes-polito\" )\nsrc_datasets='/kaggle/input/cityscapes-polito/datasets/'\ndst_datasets='/kaggle/working/datasets/'\ncopytree(src_datasets, dst_datasets)\n\nsrc_models='/kaggle/input/cityscapes-polito/models/'\ndst_models='/kaggle/working/models/'\ncopytree(src_models, dst_models)\n\ncopyfile(src = \"/kaggle/input/cityscapes-polito/utils.py\", dst = \"/kaggle/working/utils.py\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-10T08:02:48.887610Z","iopub.execute_input":"2024-05-10T08:02:48.888422Z","iopub.status.idle":"2024-05-10T08:02:49.210264Z","shell.execute_reply.started":"2024-05-10T08:02:48.888382Z","shell.execute_reply":"2024-05-10T08:02:49.208725Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m src_datasets\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/cityscapes-polito/datasets/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m dst_datasets\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/datasets/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mcopytree\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_datasets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m src_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/cityscapes-polito/models/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m dst_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/models/\u001b[39m\u001b[38;5;124m'\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:559\u001b[0m, in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(src) \u001b[38;5;28;01mas\u001b[39;00m itr:\n\u001b[1;32m    558\u001b[0m     entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itr)\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_copytree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mignore_dangling_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_dangling_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdirs_exist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirs_exist_ok\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:457\u001b[0m, in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     ignored_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 457\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirs_exist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m errors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    459\u001b[0m use_srcentry \u001b[38;5;241m=\u001b[39m copy_function \u001b[38;5;129;01mis\u001b[39;00m copy2 \u001b[38;5;129;01mor\u001b[39;00m copy_function \u001b[38;5;129;01mis\u001b[39;00m copy\n","File \u001b[0;32m/opt/conda/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n","\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/kaggle/working/datasets/'"],"ename":"FileExistsError","evalue":"[Errno 17] File exists: '/kaggle/working/datasets/'","output_type":"error"}]},{"cell_type":"code","source":"!pip install -U fvcore","metadata":{"execution":{"iopub.status.busy":"2024-05-10T08:02:51.595828Z","iopub.execute_input":"2024-05-10T08:02:51.596647Z","iopub.status.idle":"2024-05-10T08:03:10.313615Z","shell.execute_reply.started":"2024-05-10T08:02:51.596616Z","shell.execute_reply":"2024-05-10T08:03:10.312661Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting fvcore\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (6.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore) (4.66.1)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (2.4.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from fvcore) (9.5.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore) (0.9.0)\nCollecting iopath>=0.1.7 (from fvcore)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (4.9.0)\nCollecting portalocker (from iopath>=0.1.7->fvcore)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: fvcore, iopath\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=4e580fafe1d1cf0fb4341c93e7e3e1490e8338dfebab6465ca070b8b17f51960\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=7fbff70aac8f5b22a06593cea0710653fa692286b02f2c3da71b29c76a5632f5\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built fvcore iopath\nInstalling collected packages: yacs, portalocker, iopath, fvcore\nSuccessfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n","output_type":"stream"}]},{"cell_type":"code","source":"# TODO: Define here your training and validation loops.\n#models for normal jupyter \n#from datasets.cityscapes import CityScapes\n#from models.bisenet.build_bisenet import BiSeNet\n#from utils_semantic_segmentation.utils import poly_lr_scheduler\nimport sys\nsys.path.append('/kaggle/input/cityscapes-polito/datasets/') \nfrom cityscapes import CityScapes\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch\nimport numpy as np\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom models.bisenet.build_bisenet import BiSeNet\nfrom utils import poly_lr_scheduler\n\ndef mIoU(pred_mask, mask, smooth=1e-10, n_classes=19):\n    with torch.no_grad():\n        pred_mask = F.softmax(pred_mask, dim=1)\n        pred_mask = torch.argmax(pred_mask, dim=1)\n        pred_mask = pred_mask.contiguous().view(-1)\n        mask = mask.contiguous().view(-1)\n\n        iou_per_class = []\n        for clas in range(0, n_classes): #loop per pixel class\n            true_class = pred_mask == clas\n            true_label = mask == clas\n\n            if true_label.long().sum().item() == 0: #no exist label in this loop\n                iou_per_class.append(np.nan)\n            else:\n                #print(\"true class size\",true_class.shape)\n                #print(\"true label size\",true_label.shape)\n                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n                union = torch.logical_or(true_class, true_label).sum().float().item()\n\n                iou = (intersect + smooth) / (union +smooth)\n                iou_per_class.append(iou)\n        return np.nanmean(iou_per_class)\ndef pixel_accuracy(output, mask):\n    with torch.no_grad():\n        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n        correct = torch.eq(output, mask).int()\n        accuracy = float(correct.sum()) / float(correct.numel())\n    return accuracy\n\ndef convert_tensor_to_image(tensor):\n    image = tensor.permute(1, 2, 0)\n    return image\ndef train(model,optimizer, train_loader, criterion):\n    model.train()\n    running_loss = 0.0\n    iou_score=0.0\n    accuracy=0.0\n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        inputs = inputs.float()\n        targets = targets.squeeze()\n        #Compute prediction and loss\n        outputs,_,_ = model(inputs)\n        print(batch_idx)\n        \n        loss = loss_fn(outputs.to(dtype=torch.float32), targets.to(dtype=torch.int64))\n        iou_score += mIoU(outputs.to(device), targets.to(device))\n        accuracy += pixel_accuracy(outputs.to(device), targets.to(device))\n        #BackPropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n\n    train_loss = running_loss / len(train_loader)\n    iou_score = iou_score / len(train_loader)\n    accuracy = accuracy / len(train_loader)\n    return train_loss,iou_score,accuracy\n\n# Test loop\n# calculate_label_prediction is a flag used to decide wether to calculate or not ground_truth and predicted tensor\ndef test(model, test_loader, loss_fn):\n    model.eval()\n    test_loss = 0\n    iou_score=0.0\n    accuracy=0.0\n    with torch.no_grad():\n        for batch_idx,(inputs, targets) in enumerate(test_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            inputs = inputs.float()\n            targets = targets.int()\n            #Compute prediction and loss\n            outputs = model(inputs)\n            print(batch_idx)\n            loss = loss_fn(outputs.to(dtype=torch.float32), targets.squeeze().to(dtype=torch.int64))\n            iou_score += mIoU(outputs.to(device), targets.to(device))\n            accuracy += pixel_accuracy(outputs.to(device), targets.to(device))\n            test_loss += loss.item()\n    test_loss = test_loss / len(test_loader)\n    iou_score = iou_score / len(test_loader)\n    accuracy = accuracy / len(test_loader)\n    #test_accuracy = 100. * correct / total\n    return test_loss,iou_score,accuracy\n\n\ndataset_path='/kaggle/input/cityscapes-polito/Cityscapes/Cityscapes/Cityspaces/'\nannotation_train=dataset_path+'gtFine/train'\nimage_train=dataset_path+'images/train'\n\nannotation_val=dataset_path+'gtFine/val'\nimage_val=dataset_path+'images/val'\nresize_transform = transforms.Resize(interpolation=transforms.InterpolationMode.NEAREST_EXACT,size = (512,1024))\n# Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#device = \"cpu\"\ncityscapes_train = CityScapes(annotations_dir=annotation_train, images_dir=image_train,transform=resize_transform)\ncityscapes_val = CityScapes(annotations_dir=annotation_val, images_dir=image_val,transform=resize_transform)\n\ntrain_loader = DataLoader(cityscapes_train, batch_size=16, shuffle=True)\nval_loader = DataLoader(cityscapes_val, batch_size=16, shuffle=True)\n\n# Define the model and load it to the device\nbisenet = BiSeNet(num_classes=19, context_path='resnet18')\nbisenet.to(device)\noptimizer = torch.optim.Adam(bisenet.parameters(), lr=0.001)\npoly_lr_scheduler(optimizer, 0.01, 1, lr_decay_iter=1, max_iter=300, power=0.9)\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=255)\nprint(cityscapes_train.__len__())\nepoch_beginning=0\nepochs = 50","metadata":{"execution":{"iopub.status.busy":"2024-05-10T08:22:06.240188Z","iopub.execute_input":"2024-05-10T08:22:06.241199Z","iopub.status.idle":"2024-05-10T08:22:17.550806Z","shell.execute_reply.started":"2024-05-10T08:22:06.241155Z","shell.execute_reply":"2024-05-10T08:22:17.549845Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"1572\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Step 2: load model with weights (do it if you want to continue another training","metadata":{}},{"cell_type":"code","source":"#####\n#####\n#important use this block just if you want to start training from specific weight\nversion=37\npath_weights=f\"/kaggle/input/bisenet-epoch-37/bisenet_epoch_37_weights.pth\"\nbisenet = BiSeNet(num_classes=19, context_path='resnet18')\nbisenet.to(device)\n#bisenet.load_state_dict(torch.load('/kaggle/input/cityscapes-polito/bisenet_epoch_9_weights.pth'))\nbisenet.load_state_dict(torch.load(path_weights))\nepoch_beginning=version+1\nepochs = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: plot sample from dataset","metadata":{}},{"cell_type":"code","source":"plot_loader = DataLoader(cityscapes_train, batch_size=16, shuffle=True)\n(input,output)=next(iter(plot_loader))\noutput.size()\nfig, axes = plt.subplots(2, 1)\ninput_transpose=convert_tensor_to_image(input[0])\noutput_transpose=convert_tensor_to_image(output[0])\naxes[0].imshow(input_transpose)\naxes[1].imshow(output_transpose)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculate FLOPS and number of parameters","metadata":{}},{"cell_type":"code","source":"from fvcore.nn import FlopCountAnalysis, flop_count_table\n\n# -----------------------------\n# Initizialize your model here\n# -----------------------------\nplot_loader = DataLoader(cityscapes_train, batch_size=16, shuffle=True)\n(input,output)=next(iter(plot_loader))\nheight = 512\nwidth = 1024\n\nflops = FlopCountAnalysis(bisenet, input.to(device,dtype=torch.float32))\nprint(flop_count_table(flops))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## latency and FPS","metadata":{}},{"cell_type":"code","source":"import time \nimport numpy as np\nmodel=bisenet\nmodel.eval()\nfps_loader = DataLoader(cityscapes_train, batch_size=1, shuffle=True)\n(inputs, annotations) = next(iter(fps_loader))\ninputs=inputs.to(device,dtype=torch.float32)\niterations=1000\nlatency=np.empty(0)\nFPS=np.empty(0)\nfor i in range(iterations):\n    start=time.time()\n    output=model(inputs)\n    end=time.time()\n    latency_i=end-start\n    #print(latency_i)\n    latency=np.append(latency,latency_i)\n    FPS_i=float(1/latency_i)\n    FPS=np.append(FPS,FPS_i)\nmeanLatency=np.mean(latency)\nstdLatency=np.std(latency)\nmeanFPS=np.mean(FPS)\nstdFPS=np.std(FPS)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (f\"mean latency: {meanLatency} seconds\")\nprint(f\"std latency: {stdLatency} seconds\")\nprint (f\"mean FPS: {meanFPS} fps\")\nprint(f\"std FPS: {stdFPS} fps\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4 training loop","metadata":{}},{"cell_type":"code","source":"epoch_beginning=0\nepochs = 50\ntrain_iou_list=[]\ntrain_acc_list=[]\ntrain_loss_list=[]\n\ntest_iou_list=[]\ntest_acc_list=[]\ntest_loss_list=[]\nfor epoch in range(epoch_beginning,epochs):\n    train_loss,train_iou,train_acc=train(bisenet, optimizer, train_loader, loss_fn)\n    train_iou_list.append(train_iou)\n    train_acc_list.append(train_acc)\n    train_loss_list.append(train_loss)\n    file_name='bisenet_epoch_'+str(epoch)+'_weights.pth'\n    torch.save(bisenet.state_dict(),file_name)\n    test_loss,test_iou,test_acc = test(bisenet, val_loader, loss_fn)\n    test_iou_list.append(test_iou)\n    test_acc_list.append(test_acc)\n    test_loss_list.append(test_loss)\n    print(f\"Epoch n.{epoch} - Test loss: {test_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-10T08:22:17.552579Z","iopub.execute_input":"2024-05-10T08:22:17.553040Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nEpoch n.0 - Test loss: 3.1807849407196045\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nEpoch n.1 - Test loss: 1.7808082960546017\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nEpoch n.2 - Test loss: 0.7074834201484919\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nEpoch n.3 - Test loss: 2.6728300750255585\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nEpoch n.4 - Test loss: 0.8784858323633671\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nEpoch n.5 - Test loss: 0.7314360812306404\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nEpoch n.6 - Test loss: 0.5961388796567917\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nEpoch n.7 - Test loss: 2.9570942372083664\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nEpoch n.8 - Test loss: 0.6404739562422037\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nEpoch n.9 - Test loss: 0.49864338990300894\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nEpoch n.10 - Test loss: 0.4886805210262537\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Check unique values snippet","metadata":{}},{"cell_type":"code","source":"from matplotlib import cm\nimport torch\nfrom torchvision.io import read_image \nfrom PIL import Image \nimport numpy as np\nimport matplotlib.pyplot as plt\ntest_path='/kaggle/input/cityscapes-polito/Cityscapes/Cityscapes/Cityspaces/gtFine/train/hanover/hanover_000000_000164_gtFine_labelTrainIds.png'\nimage = Image.open(test_path)\n    #if(key%100==0): print(key)\nimage_array = np.array(image)\nunique_values = np.unique(image_array)\ntest=read_image(test_path)\nfig, axes = plt.subplots(1, 1)\nnew_inferno = cm.get_cmap('hsv', 13)\naxes.imshow(test.squeeze(0))\n#cmap = plt.get_cmap('bwr')\n#plt.set_cmap(cmap)\nplt.show()\nprint(unique_values)\nprint(torch.unique(test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Single Prediction Snippet","metadata":{}},{"cell_type":"code","source":"from torchvision.io import read_image \nimport posixpath\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\ndataset_path='/kaggle/input/cityscapes-polito/Cityscapes/Cityscapes/Cityspaces/'\nannotation_val=dataset_path+'gtFine/val'\ndef make_prediction(model,image_path):\n    # set model to evaluation mode\n    iou_score=0\n    accuracy=0\n    model.eval()\n    #retrieve image and annotation\n    image = read_image(image_path)\n    print(\"image size\",image.shape)\n    path=image_path.split('/')\n    image_name = posixpath.join(path[-2],path[-1])\n    annotation_path = posixpath.join(annotation_val, image_name.replace(\"_leftImg8bit.png\",\"_gtFine_labelTrainIds.png\"))\n    annotation = read_image(annotation_path)[0:3,:,:]\n    print(\"size\",annotation.shape)\n    input = resize_transform(image)\n    annotation = resize_transform(annotation)\n    #annotation=annotation.permute(1, 2, 0)\n    #annotation_encoded=RGBtoOneHot(annotation,colorDict)\n    annotation_encoded=annotation\n    \n    #generate prediction\n    with torch.no_grad():\n        plot_loader = DataLoader(cityscapes_train, batch_size=16, shuffle=False)\n        (input_dl,annotation_dl)=next(iter(plot_loader))\n        input_dl, annotation_dl = input_dl.to(device), annotation_dl.to(device)\n        #input=image\n        input=input.float().to(device)\n        input_dl=input_dl.float().to(device)\n        print(\"\")\n        print(\"generating prediction..\")\n        #we add unsqueezeto create a batch dimension\n        print(\"input size\",input.shape)\n        print(\"input_dl size\",input_dl.shape)\n        output = model(input.unsqueeze(0))\n        output_dl = model(input_dl)\n        print(\"input size\",output.shape)\n        print(\"output_dl size\",output_dl.shape)\n        print(\"annotation_dl size\",annotation_dl.shape)\n        annotation_dl=annotation_dl.squeeze()\n        #print(\"output\",output.shape)\n        #_, preds = torch.max(outputs, 1)\n        #loss = loss_fn(output_dl.to(dtype=torch.float64).to(device), annotation_dl.to(dtype=torch.int64).to(device))\n        loss = loss_fn(output.to(dtype=torch.float64).to(device), annotation_encoded.to(dtype=torch.int64).to(device))\n        iou_score += mIoU(output.to(device), annotation_encoded.to(device))\n        accuracy += pixel_accuracy(output.to(device), annotation_encoded.to(device))\n        softmax = nn.Softmax(dim=1)\n        preds = torch.argmax(softmax(output),axis=1)\n    return input,image,annotation,annotation_encoded,preds,iou_score,accuracy\n    \n\t# turn off gradient tracking\n\t\ninput,image,annotation,annotation_encoded,preds,iou_score,accuracy=make_prediction(bisenet,'/kaggle/input/cityscapes-polito/Cityscapes/Cityscapes/Cityspaces/images/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"annotation size after permutation\",annotation.shape)\nprint(\"annotation encoded size\",annotation_encoded.shape)\n#print(\"prediction size\",output.shape)\nprint(\"prediction size after softmax\",preds.shape)\n#visualization\npreds_custom=preds.squeeze()\nprint(\"scueezed preds size\",preds_custom.shape)\nfig, axes = plt.subplots(4, 1)\nfig.tight_layout()\n#axes[0].imshow(image.permute(1, 2, 0))\naxes[0].imshow(resize_transform(image).permute(1, 2, 0).cpu())\naxes[1].imshow(annotation_encoded.squeeze().cpu())\naxes[2].imshow(annotation.squeeze().cpu())\naxes[3].imshow(preds_custom.cpu())\naxes[0].set_title('Image',fontsize=10)\naxes[1].set_title('Annotation encoded',fontsize=10)\naxes[2].set_title('Annotation',fontsize=10)\naxes[3].set_title('prediction',fontsize=10)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy,iou_score)\nprint('bisenet_epoch_37_weights')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_unique_values = set()\n\nfor image_path in cityscapes_train.map_index_to_annotation:\n    #image = Image.open(image_path)\n    #if(key%100==0): print(key)\n    #image_array = np.array(image)\n    test=read_image(image_path)\n    unique_values = torch.unique(test)\n    all_unique_values.update(unique_values)\n\nprint(sorted(all_unique_values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}