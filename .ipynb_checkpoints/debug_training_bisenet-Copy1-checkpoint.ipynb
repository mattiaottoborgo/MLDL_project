{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575c66d4-dfa8-4c6b-992f-77dee7e9723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1572\n",
      "1572\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define here your training and validation loops.\n",
    "from datasets.cityscapes import CityScapes\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from models.bisenet.build_bisenet import BiSeNet\n",
    "from utils import poly_lr_scheduler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def intersectionAndUnionGPU(output, target, K, ignore_index=255):\n",
    "    # 'K' classes, output and target sizes are N or N * L or N * H * W, each value in range 0 to K - 1.\n",
    "    assert (output.dim() in [1, 2, 3])\n",
    "    assert output.shape == target.shape\n",
    "    output = output.view(-1)\n",
    "    target = target.view(-1)\n",
    "    #next line is to ignore areas that are classified as void\n",
    "    output[target == ignore_index] = ignore_index\n",
    "    intersection = output[output == target]\n",
    "    #TODO: take each pixel (r,g,b) and convert to a single value representing the class.\n",
    "    #TODO: Then, calculate the intersection, union and target areas.\n",
    "    \n",
    "    area_intersection = torch.histc(intersection, bins=K, min=0, max=K-1)\n",
    "    area_output = torch.histc(output, bins=K, min=0, max=K-1)\n",
    "    area_target = torch.histc(target, bins=K, min=0, max=K-1)\n",
    "    area_union = area_output + area_target - area_intersection\n",
    "    return area_intersection, area_union, area_target\n",
    "\n",
    "def train(model,optimizer, train_loader, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.float()\n",
    "        #targets = targets.int()\n",
    "        \n",
    "        #Compute prediction and loss\n",
    "        outputs,_,_ = model(inputs)\n",
    "        print(batch_idx)\n",
    "        #print(\"size targets before\",targets.size())\n",
    "        \"\"\"\n",
    "        encoded_torch=[]\n",
    "        op=targets.permute(0,2,3,1)\n",
    "        #print(\"op\",op.size())\n",
    "        for i, x in enumerate(op):\n",
    "            x=x.to(dtype=torch.int64)\n",
    "            partial_encoded=RGBtoOneHot(x,colorDict)\n",
    "            #print(\"partial\",partial_encoded)\n",
    "            #for numpy\n",
    "            #partial_encoded=RGBtoOneHot(x,colorDict)\n",
    "            #partial_encoded_torch=torch.from_numpy(partial_encoded)\n",
    "            #print(partial_encoded_torch.shape)\n",
    "            partial_encoded_torch =  partial_encoded.unsqueeze(0)\n",
    "            if i==0:\n",
    "                encoded_torch=partial_encoded_torch\n",
    "            else:\n",
    "                encoded_torch=torch.cat((encoded_torch,partial_encoded_torch))\n",
    "        #print(encoded_torch[0,:,:])\n",
    "        #print(\"size targets\",encoded_torch.size())\n",
    "        #print(\"size outputs\",outputs.size())\n",
    "        #preds = preds.int()\n",
    "        \"\"\"\n",
    "        unique_values = torch.unique(targets)\n",
    "        print(\"target labels\",unique_values)\n",
    "        loss = loss_fn(outputs.to(dtype=torch.float64), targets.squeeze().to(dtype=torch.int64))\n",
    "        #loss = criterion (outputs,targets)\n",
    "\n",
    "        #BackPropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        #total += targets.size(0)\n",
    "        #correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    #train_accuracy = 100. * correct / total\n",
    "    #print(f'Train Epoch: {epoch} Loss: {train_loss:.6f} Acc: {train_accuracy:.2f}%')\n",
    "    return train_loss\n",
    "\n",
    "# Test loop\n",
    "# calculate_label_prediction is a flag used to decide wether to calculate or not ground_truth and predicted tensor\n",
    "def test(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx,(inputs, targets) in enumerate(test_loader):\n",
    "            #ground_truth.append(targets)\n",
    "            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs = inputs.float()\n",
    "            #targets = targets.int()\n",
    "            \n",
    "            #Compute prediction and loss\n",
    "            outputs = model(inputs)\n",
    "            print(batch_idx)\n",
    "            #print(\"size targets before\",targets.size())\n",
    "            encoded_torch=[]\n",
    "            op=targets.permute(0,2,3,1)\n",
    "            #print(\"op\",op.size())\n",
    "            for i, x in enumerate(op):\n",
    "                x=x.to(dtype=torch.int64)\n",
    "                partial_encoded=RGBtoOneHot(x,colorDict)\n",
    "                #print(\"partial\",partial_encoded)\n",
    "                #for numpy\n",
    "                #partial_encoded=RGBtoOneHot(x,colorDict)\n",
    "                #partial_encoded_torch=torch.from_numpy(partial_encoded)\n",
    "                #print(partial_encoded_torch.shape)\n",
    "                partial_encoded_torch =  partial_encoded.unsqueeze(0)\n",
    "                if i==0:\n",
    "                    encoded_torch=partial_encoded_torch\n",
    "                else:\n",
    "                    encoded_torch=torch.cat((encoded_torch,partial_encoded_torch))\n",
    "            #print(encoded_torch[0,:,:])\n",
    "            #print(\"size targets\",encoded_torch.size())\n",
    "            #print(\"size outputs\",outputs.size())\n",
    "            #preds = preds.int()\n",
    "            loss = loss_fn(outputs.to(dtype=torch.float64), encoded_torch.to(dtype=torch.int64))\n",
    "            #loss = loss_fn(outputs, targets)\n",
    "    \n",
    "            test_loss += loss.item()\n",
    "            #probability, predicted = outputs.max(1)\n",
    "            #We need to convert to probabilities with softmax\n",
    "            #soft_outputs = torch.nn.functional.softmax(outputs, dim=1) #pass through softmax\n",
    "            #probability, predicted = soft_outputs.topk(1, dim = 1) # select top probability as prediction\n",
    "            #probability=torch.squeeze(probability)\n",
    "            #predicted=torch.squeeze(predicted)\n",
    "            #total += targets.size(0)\n",
    "            #correct += predicted.eq(targets).sum().item()\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    #test_accuracy = 100. * correct / total\n",
    "    return test_loss\n",
    "\n",
    "dataset_path='datasets/Cityscapes/Cityscapes/Cityspaces/'\n",
    "annotation_train=dataset_path+'gtFine/train'\n",
    "image_train=dataset_path+'images/train'\n",
    "\n",
    "annotation_val=dataset_path+'gtFine/val'\n",
    "image_val=dataset_path+'images/val'\n",
    "\n",
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = \"cpu\"\n",
    "cityscapes_train = CityScapes(annotations_dir=annotation_train, images_dir=image_train,transform=transforms.Resize(size = (512,1024)))\n",
    "cityscapes_val = CityScapes(annotations_dir=annotation_val, images_dir=image_val,transform=transforms.Resize(size = (512,1024)))\n",
    "\n",
    "train_loader = DataLoader(cityscapes_train, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(cityscapes_val, batch_size=4, shuffle=True)\n",
    "\n",
    "# Define the model and load it to the device\n",
    "bisenet = BiSeNet(num_classes=19, context_path='resnet18')\n",
    "bisenet.to(device)\n",
    "epochs = 10\n",
    "optimizer = torch.optim.Adam(bisenet.parameters(), lr=0.001)\n",
    "poly_lr_scheduler(optimizer, 0.01, 1, lr_decay_iter=1, max_iter=300, power=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
    "print(cityscapes_train.__len__())\n",
    "print(len(cityscapes_train.map_index_to_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d1a23-6e87-4dbe-ab9a-73c5c683d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=19):\n",
    "    with torch.no_grad():\n",
    "        print(\"pred_mask size\",pred_mask.shape)\n",
    "        print(\"mask size\",mask.shape)\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                #print(\"true class size\",true_class.shape)\n",
    "                #print(\"true label size\",true_label.shape)\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)\n",
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy\n",
    "\n",
    "def convert_tensor_to_image(tensor):\n",
    "    image = tensor.permute(1, 2, 0)\n",
    "    return image\n",
    "def RGBtoOneHot(rgb, colorDict):\n",
    "  #arr = np.zeros(rgb.shape[:2],dtype=np.int32) ## rgb shape: (h,w,3); arr shape: (h,w)\n",
    "    arr_torch=torch.zeros(rgb.to(device).shape[:2],dtype=torch.int32).to(device)\n",
    "    for label, color in enumerate(colorDict.keys()):\n",
    "    #color_np = np.array(color)\n",
    "        color_torch=torch.tensor(color).to(device)\n",
    "        if label < len(colorDict.keys()):\n",
    "      #arr[np.all(rgb[:,:,:2].to('cpu').numpy() == color_np, axis=-1)] = label\n",
    "      #arr[np.all(rgb[:,:,:3].to('cpu').numpy() == color_np, axis=-1)] = colorDict[color][1] #1 = 'id'\n",
    "            arr_torch[torch.all(rgb[:,:,:3].to(device) == color_torch, axis=-1)] = colorDict[color][2] #1 = 'id'\n",
    "    return arr_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd063aa-2d19-4250-900f-32198e908a6f",
   "metadata": {},
   "source": [
    "# PLotting dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b5084-0c70-4840-8ff4-6043793c6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "(input,output)=next(iter(train_loader))\n",
    "output.size()\n",
    "fig, axes = plt.subplots(2, 1)\n",
    "input_transpose=convert_tensor_to_image(input[0])\n",
    "output_transpose=convert_tensor_to_image(output[0])\n",
    "axes[0].imshow(input_transpose)\n",
    "axes[1].imshow(output_transpose)\n",
    "plt.show()\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90625577-c590-4e26-8a70-656fc11ae96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Label = namedtuple('Label', \n",
    "['name','id','trainId','category',\n",
    "'categoryId','hasInstances',\n",
    "'ignoreInEval','color',])\n",
    "labels = [\n",
    "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) )]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa68eb-84c6-4cc6-84cf-0cb0288df97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colorDict = {label.color: label for label in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0640c-7b23-477b-b973-0900dbb4cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def RGBtoOneHot(rgb, colorDict):\n",
    "  #arr = np.zeros(rgb.shape[:2],dtype=np.int32) ## rgb shape: (h,w,3); arr shape: (h,w)\n",
    "  arr_torch=torch.zeros(rgb.to(device).shape[:2],dtype=torch.int32).to(device)\n",
    "  for label, color in enumerate(colorDict.keys()):\n",
    "    #color_np = np.array(color)\n",
    "    color_torch=torch.tensor(color).to(device)\n",
    "    if label < len(colorDict.keys()):\n",
    "      #arr[np.all(rgb[:,:,:2].to('cpu').numpy() == color_np, axis=-1)] = label\n",
    "      #arr[np.all(rgb[:,:,:3].to('cpu').numpy() == color_np, axis=-1)] = colorDict[color][1] #1 = 'id'\n",
    "      arr_torch[torch.all(rgb[:,:,:3].to(device) == color_torch, axis=-1)] = colorDict[color][2] #1 = 'id'\n",
    "  return arr_torch\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b816f92-6834-467d-a1bc-59a8ab700ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RGBtoOneHot(output_transpose,colorDict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7d9de-d285-4f6a-8295-55bcf63c1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load the model\n",
    "bisenet = BiSeNet(num_classes=19, context_path='resnet18')\n",
    "bisenet.to(device)\n",
    "bisenet.load_state_dict(torch.load('bisenet_epoch_37_weights.pth'))\n",
    "epoch_beginning=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa98c37-016a-4bb3-96d0-0a086b733df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0,epochs):\n",
    "    train_loss=train(bisenet, optimizer, train_loader, loss_fn)\n",
    "    file_name='bisenet_epoch_'+str(epoch)+'_weights.pth'\n",
    "    torch.save(bisenet.state_dict(),file_name)\n",
    "    test_loss = test(bisenet, val_loader, loss_fn)\n",
    "    #print(\"size ground truth: \",ground_truth.size())\n",
    "    #print(f\"Epoch n.{epoch} - Test accuracy: {test_acc}\")  # You should get values around 90% accuracy on the test set\n",
    "    print(f\"Epoch n.{epoch} - Test loss: {test_loss}\")  # You should get values around 90% accuracy on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb862665-58ec-4a57-8fc8-436864698688",
   "metadata": {},
   "source": [
    "# make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7e02c-ad14-421d-9fca-f27aec159de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image \n",
    "import posixpath\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "dataset_path='datasets/Cityscapes/Cityscapes/Cityspaces/'\n",
    "annotation_val=dataset_path+'gtFine/val'\n",
    "transform = transforms.Resize((512,512))\n",
    "def make_prediction(model,image_path):\n",
    "    # set model to evaluation mode\n",
    "    iou_score=0\n",
    "    accuracy=0\n",
    "    model.eval()\n",
    "    #retrieve image and annotation\n",
    "    image = read_image(image_path)\n",
    "    print(\"image size\",image.shape)\n",
    "    path=image_path.split('/')\n",
    "    image_name = posixpath.join(path[-2],path[-1])\n",
    "    annotation_path = posixpath.join(annotation_val, image_name.replace(\"_leftImg8bit.png\",\"_gtFine_color.png\"))\n",
    "    annotation = read_image(annotation_path)[0:3,:,:]\n",
    "    input = transform(image)\n",
    "    annotation = transform(annotation)\n",
    "    annotation=annotation.permute(1, 2, 0)\n",
    "    annotation_encoded=RGBtoOneHot(annotation,colorDict)\n",
    "    \n",
    "    #generate prediction\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #input=image\n",
    "        input=input.float().to(device)\n",
    "        print(\"\")\n",
    "        print(\"generating prediction..\")\n",
    "        #we add unsqueezeto create a batch dimension\n",
    "        output = model(input.unsqueeze(0))\n",
    "        #print(\"output\",output.shape)\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        iou_score += mIoU(output, annotation_encoded)\n",
    "        accuracy += pixel_accuracy(output, annotation_encoded)\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        preds = torch.argmax(softmax(output),axis=1)\n",
    "    return input,image,annotation,annotation_encoded,preds,iou_score,accuracy\n",
    "    \n",
    "\t# turn off gradient tracking\n",
    "\t\n",
    "input,image,annotation,annotation_encoded,preds,iou_score,accuracy=make_prediction(bisenet,'datasets/Cityscapes/Cityscapes/Cityspaces/images/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4076c-b5be-4039-b002-17818ccd33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"annotation size after permutation\",annotation.shape)\n",
    "print(\"annotation encoded size\",annotation_encoded.shape)\n",
    "#print(\"prediction size\",output.shape)\n",
    "print(\"prediction size after softmax\",preds.shape)\n",
    "#visualization\n",
    "preds_custom=preds.squeeze()\n",
    "print(\"scueezed preds size\",preds_custom.shape)\n",
    "fig, axes = plt.subplots(4, 1)\n",
    "fig.tight_layout()\n",
    "#axes[0].imshow(image.permute(1, 2, 0))\n",
    "axes[0].imshow(transform(image).permute(1, 2, 0).cpu())\n",
    "axes[1].imshow(annotation_encoded.cpu())\n",
    "axes[2].imshow(annotation)\n",
    "axes[3].imshow(preds_custom.cpu())\n",
    "axes[0].set_title('Image',fontsize=10)\n",
    "axes[1].set_title('Annotation encoded',fontsize=10)\n",
    "axes[2].set_title('Annotation',fontsize=10)\n",
    "axes[3].set_title('prediction',fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e714988-8344-48c4-b07e-ebd6d4b725d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy,iou_score)\n",
    "print('bisenet_epoch_37_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2291158-6934-4d9b-bd6c-d4124ef59110",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98edb9d1-19c3-43af-b313-caae203d5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b266037-fd95-4172-b6ea-4a473d1d9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from torchvision.io import read_image \n",
    "from PIL import Image \n",
    "test_path='datasets/Cityscapes/Cityscapes/Cityspaces/gtFine/train/hanover/hanover_000000_000164_gtFine_labelTrainIds.png'\n",
    "image = Image.open(test_path)\n",
    "    #if(key%100==0): print(key)\n",
    "image_array = np.array(image)\n",
    "unique_values = np.unique(image_array)\n",
    "test=read_image(test_path)\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "new_inferno = cm.get_cmap('hsv', 13)\n",
    "axes.imshow(test.squeeze(0))\n",
    "#cmap = plt.get_cmap('bwr')\n",
    "#plt.set_cmap(cmap)\n",
    "plt.show()\n",
    "print(unique_values)\n",
    "print(torch.unique(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1518d-d6d8-41bc-bd96-c70341273fd1",
   "metadata": {},
   "source": [
    "# trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6db7b9-cdba-4dca-a96f-7ddda31d8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bisenet.state_dict(),file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472da5ca-edac-421a-b533-2270c4124c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset.__getitem__(0)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75e6f76-f3dd-44ee-9c81-15b84d742346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: remove from dict all the labels with train_id = 255\n",
    "dict={\n",
    "     \"128_64_128\"  :7,\n",
    "     \"244_35_232\"  :8,\n",
    "     \"70_70_70\" :11,\n",
    "     \"102_102_156\" :12,\n",
    "     \"190_153_153\" :13,\n",
    "     \"153_153_153\" :17,\n",
    "     \"250_170_30\" :19,\n",
    "     \"220_220_0\" :20,\n",
    "     \"107_142_35\" :21,\n",
    "     \"152_251_152\" :22,\n",
    "     \"70_130_180\" :23,\n",
    "     \"220_20_60\" :24,\n",
    "     \"255_0_0\" :25,\n",
    "     \"0_0_142\" :26,\n",
    "     \"0_0_70\" :27,\n",
    "     \"0_60_100\" :28,\n",
    "     \"0_80_100\" :31,\n",
    "     \"0_0_230\" :32,\n",
    "     \"119_11_32\" :33}\n",
    "\"\"\"\n",
    "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "\n",
    "        #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,(111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,(128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,(244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,(250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,(230,150,140) ),\n",
    "    Label(  'building'             , 11 ,( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,(102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,(190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,(180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,(150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,(150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,(153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,(153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,(250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,(220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,(107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,(152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,(220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,(255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,(  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,(  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,(  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,(  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,(  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,(  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,(  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,(119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,(  0,  0,142) ),\n",
    "\n",
    "     {\"111_74_0\"  :5,\n",
    "     \"81_0_81\"  :6,\n",
    "     \"128_64_128\"  :7,\n",
    "     \"244_35_232\"  :8,\n",
    "     \"250_170_160\"  :9,\n",
    "     \"230_150_140\" :10,\n",
    "     \"70_70_70\" :11,\n",
    "     \"102_102_156\" :12,\n",
    "     \"190_153_153\" :13,\n",
    "     \"180_165_180\" :14,\n",
    "     \"150_100_100\" :15,\n",
    "     \"150_120_90\" :16,\n",
    "     \"153_153_153\" :17,\n",
    "     \"153_153_153\" :18,\n",
    "     \"250_170_30\" :19,\n",
    "     \"220_220_0\" :20,\n",
    "     \"107_142_35\" :21,\n",
    "     \"152_251_152\" :22,\n",
    "     \"70_130_180\" :23,\n",
    "     \"220_20_60\" :24,\n",
    "     \"255_0_0\" :25,\n",
    "     \"0_0_142\" :26,\n",
    "     \"0_0_70\" :27,\n",
    "     \"0_60_100\" :28,\n",
    "     \"0_0_ 90\" :29,\n",
    "     \"0_0_110\" :30,\n",
    "     \"0_80_100\" :31,\n",
    "     \"0_0_230\" :32,\n",
    "     \"119_11_32\" :33,\n",
    "     \"0_0_142\" :-1}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582bdf93-7001-4320-ae84-9fbd478531be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241bf60f-3831-4709-b968-86f90007a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: crop input to 224x224\n",
    "#TODO: create dictionary to map each label to color\n",
    "#dict={\"12864128\":10} # 0=road, the key is concatenation of rgb values\n",
    "dict={\n",
    "     \"128_64_128\"  :7,\n",
    "     \"244_35_232\"  :8,\n",
    "     \"70_70_70\" :11,\n",
    "     \"102_102_156\" :12,\n",
    "     \"190_153_153\" :13,\n",
    "     \"153_153_153\" :17,\n",
    "     \"250_170_30\" :19,\n",
    "     \"220_220_0\" :20,\n",
    "     \"107_142_35\" :21,\n",
    "     \"152_251_152\" :22,\n",
    "     \"70_130_180\" :23,\n",
    "     \"220_20_60\" :24,\n",
    "     \"255_0_0\" :25,\n",
    "     \"0_0_142\" :26,\n",
    "     \"0_0_70\" :27,\n",
    "     \"0_60_100\" :28,\n",
    "     \"0_80_100\" :31,\n",
    "     \"0_0_230\" :32,\n",
    "     \"119_11_32\" :33}\n",
    "def map_value_to_label():\n",
    "    #use as key the concatenation of rgb. this gives the label\n",
    "    pass\n",
    "def map_labels(a):\n",
    "    key=str(a[0])+\"_\"+str(a[1])+\"_\"+str(a[2])\n",
    "    #label=dict.get(key)\n",
    "    #if label == None:\n",
    "    #    return -1\n",
    "    #return label\n",
    "    return key\n",
    "    #print(a[0],a[1],a[2])\n",
    "\n",
    "\"\"\"\n",
    "print(op.size())\n",
    "x=op.to('cpu').numpy()#-->(r,g,b,a)\n",
    "#y=pd.DataFrame(x)\n",
    "x.shape\n",
    "#x=x.reshape(32,1024,2048,4)\n",
    "x_rgb=x[:,:,:,0:3]\n",
    "#print(x[:,:,:,0:3])\n",
    "y=np.apply_along_axis(map_labels, -1, x_rgb)\n",
    "y.shape\n",
    "y[0][0][0]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "x[:,:,:,3]= 1\n",
    "#x[:,:,:,3] =\n",
    "from operator import itemgetter\n",
    "x[:,:,:,3] =itemgetter(str(x[:,:,:,0])+\"_\"+str(x[:,:,:,1])+\"_\"+str(x[:,:,:,2]))(dict)\n",
    "#itemgetter(\"128_64_128\",\"244_35_232\")(dict)\n",
    "#dict.get(\"128_64_128\",\"244_35_232\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5c0f0-2120-47a9-9487-73aca115059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08840312-018a-4be9-8f1b-86cdeb38e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "all_unique_values = set()\n",
    "\n",
    "for image_path in cityscapes_train.map_index_to_annotation:\n",
    "    image = Image.open(image_path)\n",
    "    #if(key%100==0): print(key)\n",
    "    image_array = np.array(image)\n",
    "    \n",
    "    unique_values = np.unique(image_array)\n",
    "    all_unique_values.update(unique_values)\n",
    "\n",
    "print(sorted(all_unique_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3693a01b-4e78-4ac0-95f3-15817f26e06c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PngImageFile' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (\u001b[38;5;28minput\u001b[39m,output)\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m unique_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(output)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(unique_values)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/MLDL/MLDL_project/datasets/cityscapes.py:64\u001b[0m, in \u001b[0;36mCityScapes.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     61\u001b[0m transform_tensor\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#annotation=transform_tensor(annotation)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#print(\"loader unicue: \",torch.unique(annotation))\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     66\u001b[0m      \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PngImageFile' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "(input,output)=next(iter(train_loader))\n",
    "unique_values = np.unique(output)\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd1767-964a-4931-9c06-61dca7d2ee64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c55db44-468a-447e-9238-782d23757e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
