{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U fvcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define here your training and validation loops.\n",
    "#models for normal jupyter \n",
    "#from datasets.cityscapes import CityScapes\n",
    "#from models.bisenet.build_bisenet import BiSeNet\n",
    "#from utils_semantic_segmentation.utils import poly_lr_scheduler\n",
    "\n",
    "from datasets.cityscapes import CityScapes\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from models.bisenet.build_bisenet import BiSeNet\n",
    "from utils import poly_lr_scheduler\n",
    "\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=19):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                #print(\"true class size\",true_class.shape)\n",
    "                #print(\"true label size\",true_label.shape)\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)\n",
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy\n",
    "\n",
    "def convert_tensor_to_image(tensor):\n",
    "    image = tensor.permute(1, 2, 0)\n",
    "    return image\n",
    "def train(model,optimizer, train_loader, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    iou_score=0.0\n",
    "    accuracy=0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.float()\n",
    "        targets = targets.squeeze()\n",
    "        #Compute prediction and loss\n",
    "        outputs,_,_ = model(inputs)\n",
    "        print(batch_idx)\n",
    "        \n",
    "        loss = loss_fn(outputs.to(dtype=torch.float32), targets.to(dtype=torch.int64))\n",
    "        iou_score += mIoU(outputs.to(device), targets.to(device))\n",
    "        accuracy += pixel_accuracy(outputs.to(device), targets.to(device))\n",
    "        #BackPropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    iou_score = iou_score / len(train_loader)\n",
    "    accuracy = accuracy / len(train_loader)\n",
    "    return train_loss,iou_score,accuracy\n",
    "\n",
    "# Test loop\n",
    "# calculate_label_prediction is a flag used to decide wether to calculate or not ground_truth and predicted tensor\n",
    "def test(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    iou_score=0.0\n",
    "    accuracy=0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx,(inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs = inputs.float()\n",
    "            targets = targets.int()\n",
    "            #Compute prediction and loss\n",
    "            outputs = model(inputs)\n",
    "            print(batch_idx)\n",
    "            loss = loss_fn(outputs.to(dtype=torch.float32), targets.squeeze().to(dtype=torch.int64))\n",
    "            iou_score += mIoU(outputs.to(device), targets.to(device))\n",
    "            accuracy += pixel_accuracy(outputs.to(device), targets.to(device))\n",
    "            test_loss += loss.item()\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    iou_score = iou_score / len(test_loader)\n",
    "    accuracy = accuracy / len(test_loader)\n",
    "    #test_accuracy = 100. * correct / total\n",
    "    return test_loss,iou_score,accuracy\n",
    "\n",
    "\n",
    "#dataset_path='/kaggle/input/cityscapes-polito/Cityscapes/Cityscapes/Cityspaces/'\n",
    "dataset_path='datasets/Cityscapes/Cityscapes/Cityspaces/'\n",
    "annotation_train=dataset_path+'gtFine/train'\n",
    "image_train=dataset_path+'images/train'\n",
    "\n",
    "annotation_val=dataset_path+'gtFine/val'\n",
    "image_val=dataset_path+'images/val'\n",
    "resize_transform = transforms.Resize(interpolation=transforms.InterpolationMode.NEAREST_EXACT,size = (512,1024))\n",
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cityscapes_train = CityScapes(annotations_dir=annotation_train, images_dir=image_train,transform=resize_transform)\n",
    "cityscapes_val = CityScapes(annotations_dir=annotation_val, images_dir=image_val,transform=resize_transform)\n",
    "\n",
    "train_loader = DataLoader(cityscapes_train, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(cityscapes_val, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the model and load it to the device\n",
    "bisenet = BiSeNet(num_classes=19, context_path='resnet18')\n",
    "bisenet.to(device)\n",
    "optimizer = torch.optim.Adam(bisenet.parameters(), lr=0.001)\n",
    "poly_lr_scheduler(optimizer, 0.01, 1, lr_decay_iter=1, max_iter=300, power=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
    "print(cityscapes_train.__len__())\n",
    "epoch_beginning=0\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: load model with weights (do it if you want to continue another training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#####\n",
    "#important use this block just if you want to start training from specific weight\n",
    "version=37\n",
    "path_weights=f\"/kaggle/input/bisenet-epoch-37/bisenet_epoch_37_weights.pth\"\n",
    "bisenet = BiSeNet(num_classes=19, context_path='resnet18')\n",
    "bisenet.to(device)\n",
    "#bisenet.load_state_dict(torch.load('/kaggle/input/cityscapes-polito/bisenet_epoch_9_weights.pth'))\n",
    "bisenet.load_state_dict(torch.load(path_weights))\n",
    "epoch_beginning=version+1\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: plot sample from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loader = DataLoader(cityscapes_train, batch_size=16, shuffle=True)\n",
    "(input,output)=next(iter(plot_loader))\n",
    "output.size()\n",
    "fig, axes = plt.subplots(2, 1)\n",
    "input_transpose=convert_tensor_to_image(input[0])\n",
    "output_transpose=convert_tensor_to_image(output[0])\n",
    "axes[0].imshow(input_transpose)\n",
    "axes[1].imshow(output_transpose)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate FLOPS and number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
    "\n",
    "# -----------------------------\n",
    "# Initizialize your model here\n",
    "# -----------------------------\n",
    "plot_loader = DataLoader(cityscapes_train, batch_size=16, shuffle=True)\n",
    "(input,output)=next(iter(plot_loader))\n",
    "height = 512\n",
    "width = 1024\n",
    "\n",
    "flops = FlopCountAnalysis(bisenet, input.to(device,dtype=torch.float32))\n",
    "print(flop_count_table(flops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## latency and FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import numpy as np\n",
    "model=bisenet\n",
    "model.eval()\n",
    "fps_loader = DataLoader(cityscapes_train, batch_size=1, shuffle=True)\n",
    "(inputs, annotations) = next(iter(fps_loader))\n",
    "inputs=inputs.to(device,dtype=torch.float32)\n",
    "iterations=1000\n",
    "latency=np.empty(0)\n",
    "FPS=np.empty(0)\n",
    "for i in range(iterations):\n",
    "    start=time.time()\n",
    "    output=model(inputs)\n",
    "    end=time.time()\n",
    "    latency_i=end-start\n",
    "    #print(latency_i)\n",
    "    latency=np.append(latency,latency_i)\n",
    "    FPS_i=float(1/latency_i)\n",
    "    FPS=np.append(FPS,FPS_i)\n",
    "meanLatency=np.mean(latency)\n",
    "stdLatency=np.std(latency)\n",
    "meanFPS=np.mean(FPS)\n",
    "stdFPS=np.std(FPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"mean latency: {meanLatency} seconds\")\n",
    "print(f\"std latency: {stdLatency} seconds\")\n",
    "print (f\"mean FPS: {meanFPS} fps\")\n",
    "print(f\"std FPS: {stdFPS} fps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_beginning=0\n",
    "epochs = 50\n",
    "train_iou_list=[]\n",
    "train_acc_list=[]\n",
    "train_loss_list=[]\n",
    "\n",
    "test_iou_list=[]\n",
    "test_acc_list=[]\n",
    "test_loss_list=[]\n",
    "for epoch in range(epoch_beginning,epochs):\n",
    "    train_loss,train_iou,train_acc=train(bisenet, optimizer, train_loader, loss_fn)\n",
    "    train_iou_list.append(train_iou)\n",
    "    train_acc_list.append(train_acc)\n",
    "    train_loss_list.append(train_loss)\n",
    "    file_name='bisenet_epoch_'+str(epoch)+'_weights.pth'\n",
    "    torch.save(bisenet.state_dict(),file_name)\n",
    "    test_loss,test_iou,test_acc = test(bisenet, val_loader, loss_fn)\n",
    "    test_iou_list.append(test_iou)\n",
    "    test_acc_list.append(test_acc)\n",
    "    test_loss_list.append(test_loss)\n",
    "    print(f\"Epoch n.{epoch} - Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check unique values snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "import torch\n",
    "from torchvision.io import read_image \n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "test_path='/kaggle/input/cityscapes-polito/Cityscapes/Cityscapes/Cityspaces/gtFine/train/hanover/hanover_000000_000164_gtFine_labelTrainIds.png'\n",
    "image = Image.open(test_path)\n",
    "    #if(key%100==0): print(key)\n",
    "image_array = np.array(image)\n",
    "unique_values = np.unique(image_array)\n",
    "test=read_image(test_path)\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "new_inferno = cm.get_cmap('hsv', 13)\n",
    "axes.imshow(test.squeeze(0))\n",
    "#cmap = plt.get_cmap('bwr')\n",
    "#plt.set_cmap(cmap)\n",
    "plt.show()\n",
    "print(unique_values)\n",
    "print(torch.unique(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Prediction Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image \n",
    "import posixpath\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "dataset_path='/kaggle/input/cityscapes-polito/Cityscapes/Cityscapes/Cityspaces/'\n",
    "annotation_val=dataset_path+'gtFine/val'\n",
    "def make_prediction(model,image_path):\n",
    "    # set model to evaluation mode\n",
    "    iou_score=0\n",
    "    accuracy=0\n",
    "    model.eval()\n",
    "    #retrieve image and annotation\n",
    "    image = read_image(image_path)\n",
    "    print(\"image size\",image.shape)\n",
    "    path=image_path.split('/')\n",
    "    image_name = posixpath.join(path[-2],path[-1])\n",
    "    annotation_path = posixpath.join(annotation_val, image_name.replace(\"_leftImg8bit.png\",\"_gtFine_labelTrainIds.png\"))\n",
    "    annotation = read_image(annotation_path)[0:3,:,:]\n",
    "    print(\"size\",annotation.shape)\n",
    "    input = resize_transform(image)\n",
    "    annotation = resize_transform(annotation)\n",
    "    #annotation=annotation.permute(1, 2, 0)\n",
    "    #annotation_encoded=RGBtoOneHot(annotation,colorDict)\n",
    "    annotation_encoded=annotation\n",
    "    \n",
    "    #generate prediction\n",
    "    with torch.no_grad():\n",
    "        plot_loader = DataLoader(cityscapes_train, batch_size=16, shuffle=False)\n",
    "        (input_dl,annotation_dl)=next(iter(plot_loader))\n",
    "        input_dl, annotation_dl = input_dl.to(device), annotation_dl.to(device)\n",
    "        #input=image\n",
    "        input=input.float().to(device)\n",
    "        input_dl=input_dl.float().to(device)\n",
    "        print(\"\")\n",
    "        print(\"generating prediction..\")\n",
    "        #we add unsqueezeto create a batch dimension\n",
    "        print(\"input size\",input.shape)\n",
    "        print(\"input_dl size\",input_dl.shape)\n",
    "        output = model(input.unsqueeze(0))\n",
    "        output_dl = model(input_dl)\n",
    "        print(\"input size\",output.shape)\n",
    "        print(\"output_dl size\",output_dl.shape)\n",
    "        print(\"annotation_dl size\",annotation_dl.shape)\n",
    "        annotation_dl=annotation_dl.squeeze()\n",
    "        #print(\"output\",output.shape)\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss = loss_fn(output_dl.to(dtype=torch.float64).to(device), annotation_dl.to(dtype=torch.int64).to(device))\n",
    "        loss = loss_fn(output.to(dtype=torch.float64).to(device), annotation_encoded.to(dtype=torch.int64).to(device))\n",
    "        iou_score += mIoU(output.to(device), annotation_encoded.to(device))\n",
    "        accuracy += pixel_accuracy(output.to(device), annotation_encoded.to(device))\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        preds = torch.argmax(softmax(output),axis=1)\n",
    "    return input,image,annotation,annotation_encoded,preds,iou_score,accuracy\n",
    "    \n",
    "\t# turn off gradient tracking\n",
    "\t\n",
    "input,image,annotation,annotation_encoded,preds,iou_score,accuracy=make_prediction(bisenet,'/kaggle/input/cityscapes-polito/Cityscapes/Cityscapes/Cityspaces/images/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"annotation size after permutation\",annotation.shape)\n",
    "print(\"annotation encoded size\",annotation_encoded.shape)\n",
    "#print(\"prediction size\",output.shape)\n",
    "print(\"prediction size after softmax\",preds.shape)\n",
    "#visualization\n",
    "preds_custom=preds.squeeze()\n",
    "print(\"scueezed preds size\",preds_custom.shape)\n",
    "fig, axes = plt.subplots(4, 1)\n",
    "fig.tight_layout()\n",
    "#axes[0].imshow(image.permute(1, 2, 0))\n",
    "axes[0].imshow(resize_transform(image).permute(1, 2, 0).cpu())\n",
    "axes[1].imshow(annotation_encoded.squeeze().cpu())\n",
    "axes[2].imshow(annotation.squeeze().cpu())\n",
    "axes[3].imshow(preds_custom.cpu())\n",
    "axes[0].set_title('Image',fontsize=10)\n",
    "axes[1].set_title('Annotation encoded',fontsize=10)\n",
    "axes[2].set_title('Annotation',fontsize=10)\n",
    "axes[3].set_title('prediction',fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy,iou_score)\n",
    "print('bisenet_epoch_37_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_values = set()\n",
    "\n",
    "for image_path in cityscapes_train.map_index_to_annotation:\n",
    "    #image = Image.open(image_path)\n",
    "    #if(key%100==0): print(key)\n",
    "    #image_array = np.array(image)\n",
    "    test=read_image(image_path)\n",
    "    unique_values = torch.unique(test)\n",
    "    all_unique_values.update(unique_values)\n",
    "\n",
    "print(sorted(all_unique_values))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4967162,
     "sourceId": 8358506,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4967173,
     "sourceId": 8358521,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4951893,
     "sourceId": 8358637,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
